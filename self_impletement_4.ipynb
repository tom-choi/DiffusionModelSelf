{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.3.0+cu121 with CUDA 1201 (you have 2.4.1+cu121)\n",
      "    Python  3.10.11 (you have 3.10.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\python3.10\\lib\\site-packages\\xformers\\__init__.py\", line 55, in _is_triton_available\n",
      "    from xformers.triton.softmax import softmax as triton_softmax  # noqa\n",
      "  File \"f:\\python3.10\\lib\\site-packages\\xformers\\triton\\softmax.py\", line 11, in <module>\n",
      "    import triton\n",
      "ModuleNotFoundError: No module named 'triton'\n",
      "f:\\python3.10\\lib\\site-packages\\xformers\\ops\\swiglu_op.py:107: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(cls, ctx, x, w1, b1, w2, b2, w3, b3):\n",
      "f:\\python3.10\\lib\\site-packages\\xformers\\ops\\swiglu_op.py:128: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(cls, ctx, dx5):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from datasets import load_dataset\n",
    "from diffusers import DDIMScheduler, DDPMPipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading A Pre-Trained Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e431e881a924984830205b8468b0d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch C:\\Users\\chcbb\\.cache\\huggingface\\hub\\models--google--ddpm-celebahq-256\\snapshots\\cd5c944777ea2668051904ead6cc120739b86c4d: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\chcbb\\.cache\\huggingface\\hub\\models--google--ddpm-celebahq-256\\snapshots\\cd5c944777ea2668051904ead6cc120739b86c4d.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    }
   ],
   "source": [
    "image_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "image_pipe.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e9a204e48a4b5ba2b22cdaf222c872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python3.10\\lib\\site-packages\\diffusers\\models\\attention_processor.py:2383: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  hidden_states = F.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "images = image_pipe().images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster Sampling with DDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new scheduler and set num inference steps\n",
    "scheduler = DDIMScheduler.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "scheduler.set_timesteps(num_inference_steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The random starting point\n",
    "x = torch.randn(4, 3, 256, 256).to(device)  # Batch of 4, 3-channel 256 x 256 px images\n",
    "\n",
    "# Loop through the sampling timesteps\n",
    "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "\n",
    "    # Prepare model input\n",
    "    model_input = scheduler.scale_model_input(x, t)\n",
    "\n",
    "    # Get the prediction\n",
    "    with torch.no_grad():\n",
    "        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "\n",
    "    # Calculate what the updated sample should look like with the scheduler\n",
    "    scheduler_output = scheduler.step(noise_pred, t, x)\n",
    "\n",
    "    # Update x\n",
    "    x = scheduler_output.prev_sample\n",
    "\n",
    "    # Occasionally display both x and the predicted denoised images\n",
    "    if i % 10 == 0 or i == len(scheduler.timesteps) - 1:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "        grid = torchvision.utils.make_grid(x, nrow=4).permute(1, 2, 0)\n",
    "        axs[0].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)\n",
    "        axs[0].set_title(f\"Current x (step {i})\")\n",
    "\n",
    "        pred_x0 = (\n",
    "            scheduler_output.pred_original_sample\n",
    "        )  # Not available for all schedulers\n",
    "        grid = torchvision.utils.make_grid(pred_x0, nrow=4).permute(1, 2, 0)\n",
    "        axs[1].imshow(grid.cpu().clip(-1, 1) * 0.5 + 0.5)\n",
    "        axs[1].set_title(f\"Predicted denoised images (step {i})\")\n",
    "        plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pipe.scheduler = scheduler\n",
    "images = image_pipe(num_inference_steps=40).images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown load and prepare a dataset:\n",
    "# Not on Colab? Comments with #@ enable UI tweaks like headings or user inputs\n",
    "# but can safely be ignored if you're working on a different platform.\n",
    "\n",
    "dataset_name = \"huggan/smithsonian_butterflies_subset\"  # @param\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "image_size = 256  # @param\n",
    "batch_size = 4  # @param\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Previewing batch:\")\n",
    "batch = next(iter(train_dataloader))\n",
    "grid = torchvision.utils.make_grid(batch[\"images\"], nrow=4)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
